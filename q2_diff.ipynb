{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_transforms = [\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Lambda(lambda t: (t * 2) - 1) \n",
    "    ]\n",
    "data_transform = transforms.Compose(data_transforms)\n",
    "\n",
    "train = torchvision.datasets.CIFAR10(root=\".\", download=True,transform=data_transform, train=True)\n",
    "\n",
    "test = torchvision.datasets.CIFAR10(root=\".\", download=True,transform=data_transform, train=False)\n",
    "\n",
    "data =  torch.utils.data.ConcatDataset([train, test])\n",
    "data_loader=DataLoader(data,batch_size=128,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=300\n",
    "betas=torch.linspace(0.0001,0.02,T)\n",
    "alphas=1-betas\n",
    "\n",
    "alphas_cumprod=torch.cumprod(alphas,axis=0)\n",
    "\n",
    "\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_from_list(vals, t, x_shape):\n",
    "    \"\"\"\n",
    "    Returns a specific index t of a passed list of values vals\n",
    "    while considering the batch dimension.\n",
    "    \"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Takes an image and a timestep as input and\n",
    "    returns the noisy version of it\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
    "    )\n",
    "    # mean + variance\n",
    "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \\\n",
    "    + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, t, ):\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        h = h + time_emb\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        return self.transform(h)\n",
    "\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class SimpleUnet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        image_channels = 3\n",
    "        down_channels = (64, 128, 256, 512, 1024)\n",
    "        up_channels = (1024, 512, 256, 128, 64)\n",
    "        out_dim = 3\n",
    "        time_emb_dim = 32\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "                SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "                nn.Linear(time_emb_dim, time_emb_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], \\\n",
    "                                    time_emb_dim) \\\n",
    "                    for i in range(len(down_channels)-1)])\n",
    "\n",
    "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], \\\n",
    "                                        time_emb_dim, up=True) \\\n",
    "                    for i in range(len(up_channels)-1)])\n",
    "\n",
    "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "        t = self.time_mlp(timestep)\n",
    "        x = self.conv0(x)\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t)\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | step 000 Loss: 0.8123559951782227 \n",
      "Epoch 0 | step 001 Loss: 0.7556984424591064 \n",
      "Epoch 0 | step 002 Loss: 0.713679850101471 \n",
      "Epoch 0 | step 003 Loss: 0.6853588223457336 \n",
      "Epoch 0 | step 004 Loss: 0.6506916880607605 \n",
      "Epoch 0 | step 005 Loss: 0.6194464564323425 \n",
      "Epoch 0 | step 006 Loss: 0.5812672972679138 \n",
      "Epoch 0 | step 007 Loss: 0.546823263168335 \n",
      "Epoch 0 | step 008 Loss: 0.5183539390563965 \n",
      "Epoch 0 | step 009 Loss: 0.4960297644138336 \n",
      "Epoch 0 | step 010 Loss: 0.46571481227874756 \n",
      "Epoch 0 | step 011 Loss: 0.4388415813446045 \n",
      "Epoch 0 | step 012 Loss: 0.42291462421417236 \n",
      "Epoch 0 | step 013 Loss: 0.4054476022720337 \n",
      "Epoch 0 | step 014 Loss: 0.3996020555496216 \n",
      "Epoch 0 | step 015 Loss: 0.38702520728111267 \n",
      "Epoch 0 | step 016 Loss: 0.3871452808380127 \n",
      "Epoch 0 | step 017 Loss: 0.3791097104549408 \n",
      "Epoch 0 | step 018 Loss: 0.3561975955963135 \n",
      "Epoch 0 | step 019 Loss: 0.36527374386787415 \n",
      "Epoch 0 | step 020 Loss: 0.36259666085243225 \n",
      "Epoch 0 | step 021 Loss: 0.3482036292552948 \n",
      "Epoch 0 | step 022 Loss: 0.33732399344444275 \n",
      "Epoch 0 | step 023 Loss: 0.3390350341796875 \n",
      "Epoch 0 | step 024 Loss: 0.3191955089569092 \n",
      "Epoch 0 | step 025 Loss: 0.319019079208374 \n",
      "Epoch 0 | step 026 Loss: 0.31570717692375183 \n",
      "Epoch 0 | step 027 Loss: 0.310809463262558 \n",
      "Epoch 0 | step 028 Loss: 0.32709458470344543 \n",
      "Epoch 0 | step 029 Loss: 0.2928674519062042 \n",
      "Epoch 0 | step 030 Loss: 0.3059094250202179 \n",
      "Epoch 0 | step 031 Loss: 0.2796003520488739 \n",
      "Epoch 0 | step 032 Loss: 0.2833002507686615 \n",
      "Epoch 0 | step 033 Loss: 0.28372326493263245 \n",
      "Epoch 0 | step 034 Loss: 0.2869226038455963 \n",
      "Epoch 0 | step 035 Loss: 0.2742403745651245 \n",
      "Epoch 0 | step 036 Loss: 0.26841044425964355 \n",
      "Epoch 0 | step 037 Loss: 0.2882196307182312 \n",
      "Epoch 0 | step 038 Loss: 0.2863081395626068 \n",
      "Epoch 0 | step 039 Loss: 0.2689458429813385 \n",
      "Epoch 0 | step 040 Loss: 0.25017592310905457 \n",
      "Epoch 0 | step 041 Loss: 0.2595268189907074 \n",
      "Epoch 0 | step 042 Loss: 0.2498229295015335 \n",
      "Epoch 0 | step 043 Loss: 0.24005348980426788 \n",
      "Epoch 0 | step 044 Loss: 0.2491254359483719 \n",
      "Epoch 0 | step 045 Loss: 0.24117259681224823 \n",
      "Epoch 0 | step 046 Loss: 0.24011056125164032 \n",
      "Epoch 0 | step 047 Loss: 0.24880830943584442 \n",
      "Epoch 0 | step 048 Loss: 0.43856707215309143 \n",
      "Epoch 0 | step 049 Loss: 0.27784523367881775 \n",
      "Epoch 0 | step 050 Loss: 0.2898974120616913 \n",
      "Epoch 0 | step 051 Loss: 0.2692667841911316 \n",
      "Epoch 0 | step 052 Loss: 0.3047865033149719 \n",
      "Epoch 0 | step 053 Loss: 0.3304113447666168 \n",
      "Epoch 0 | step 054 Loss: 0.3539852201938629 \n",
      "Epoch 0 | step 055 Loss: 0.35060426592826843 \n",
      "Epoch 0 | step 056 Loss: 0.3264930546283722 \n",
      "Epoch 0 | step 057 Loss: 0.3122124969959259 \n",
      "Epoch 0 | step 058 Loss: 0.291653037071228 \n",
      "Epoch 0 | step 059 Loss: 0.26725080609321594 \n",
      "Epoch 0 | step 060 Loss: 0.26447710394859314 \n",
      "Epoch 0 | step 061 Loss: 0.2764441668987274 \n",
      "Epoch 0 | step 062 Loss: 0.2947256863117218 \n",
      "Epoch 0 | step 063 Loss: 0.29710328578948975 \n",
      "Epoch 0 | step 064 Loss: 0.2862514555454254 \n",
      "Epoch 0 | step 065 Loss: 0.306034654378891 \n",
      "Epoch 0 | step 066 Loss: 0.2849331796169281 \n",
      "Epoch 0 | step 067 Loss: 0.287337064743042 \n",
      "Epoch 0 | step 068 Loss: 0.3009640872478485 \n",
      "Epoch 0 | step 069 Loss: 0.2986868917942047 \n",
      "Epoch 0 | step 070 Loss: 0.28606167435646057 \n",
      "Epoch 0 | step 071 Loss: 0.29643043875694275 \n",
      "Epoch 0 | step 072 Loss: 0.30963605642318726 \n",
      "Epoch 0 | step 073 Loss: 0.27236485481262207 \n",
      "Epoch 0 | step 074 Loss: 0.26926249265670776 \n",
      "Epoch 0 | step 075 Loss: 0.25724172592163086 \n",
      "Epoch 0 | step 076 Loss: 0.27648061513900757 \n",
      "Epoch 0 | step 077 Loss: 0.2643270492553711 \n",
      "Epoch 0 | step 078 Loss: 0.2617015838623047 \n",
      "Epoch 0 | step 079 Loss: 0.25261422991752625 \n",
      "Epoch 0 | step 080 Loss: 0.24198025465011597 \n",
      "Epoch 0 | step 081 Loss: 0.2596082091331482 \n",
      "Epoch 0 | step 082 Loss: 0.2620070278644562 \n",
      "Epoch 0 | step 083 Loss: 0.24605651199817657 \n",
      "Epoch 0 | step 084 Loss: 0.23912054300308228 \n",
      "Epoch 0 | step 085 Loss: 0.2198398858308792 \n",
      "Epoch 0 | step 086 Loss: 0.2511114180088043 \n",
      "Epoch 0 | step 087 Loss: 0.2497292011976242 \n",
      "Epoch 0 | step 088 Loss: 0.26804518699645996 \n",
      "Epoch 0 | step 089 Loss: 0.21846377849578857 \n",
      "Epoch 0 | step 090 Loss: 0.22187475860118866 \n",
      "Epoch 0 | step 091 Loss: 0.22108809649944305 \n",
      "Epoch 0 | step 092 Loss: 0.22602184116840363 \n",
      "Epoch 0 | step 093 Loss: 0.22701828181743622 \n",
      "Epoch 0 | step 094 Loss: 0.2444884330034256 \n",
      "Epoch 0 | step 095 Loss: 0.2279941439628601 \n",
      "Epoch 0 | step 096 Loss: 0.20804445445537567 \n",
      "Epoch 0 | step 097 Loss: 0.23334556818008423 \n",
      "Epoch 0 | step 098 Loss: 0.22995245456695557 \n",
      "Epoch 0 | step 099 Loss: 0.2075899839401245 \n",
      "Epoch 0 | step 100 Loss: 0.24700850248336792 \n",
      "Epoch 0 | step 101 Loss: 0.2090880125761032 \n",
      "Epoch 0 | step 102 Loss: 0.21650302410125732 \n",
      "Epoch 0 | step 103 Loss: 0.21473002433776855 \n",
      "Epoch 0 | step 104 Loss: 0.2137811779975891 \n",
      "Epoch 0 | step 105 Loss: 0.2209564596414566 \n",
      "Epoch 0 | step 106 Loss: 0.21188409626483917 \n",
      "Epoch 0 | step 107 Loss: 0.2103026658296585 \n",
      "Epoch 0 | step 108 Loss: 0.2262762188911438 \n",
      "Epoch 0 | step 109 Loss: 0.22284717857837677 \n",
      "Epoch 0 | step 110 Loss: 0.20083339512348175 \n",
      "Epoch 0 | step 111 Loss: 0.2269091159105301 \n",
      "Epoch 0 | step 112 Loss: 0.23207928240299225 \n",
      "Epoch 0 | step 113 Loss: 0.20588035881519318 \n",
      "Epoch 0 | step 114 Loss: 0.20406295359134674 \n",
      "Epoch 0 | step 115 Loss: 0.2131822109222412 \n",
      "Epoch 0 | step 116 Loss: 0.19172559678554535 \n",
      "Epoch 0 | step 117 Loss: 0.2095022201538086 \n",
      "Epoch 0 | step 118 Loss: 0.19852370023727417 \n",
      "Epoch 0 | step 119 Loss: 0.2066716104745865 \n",
      "Epoch 0 | step 120 Loss: 0.20532481372356415 \n",
      "Epoch 0 | step 121 Loss: 0.20414364337921143 \n",
      "Epoch 0 | step 122 Loss: 0.20679044723510742 \n",
      "Epoch 0 | step 123 Loss: 0.20589637756347656 \n",
      "Epoch 0 | step 124 Loss: 0.19743724167346954 \n",
      "Epoch 0 | step 125 Loss: 0.2126873880624771 \n",
      "Epoch 0 | step 126 Loss: 0.18876399099826813 \n",
      "Epoch 0 | step 127 Loss: 0.20084695518016815 \n",
      "Epoch 0 | step 128 Loss: 0.20750929415225983 \n",
      "Epoch 0 | step 129 Loss: 0.2132198065519333 \n",
      "Epoch 0 | step 130 Loss: 0.21290023624897003 \n",
      "Epoch 0 | step 131 Loss: 0.20442764461040497 \n",
      "Epoch 0 | step 132 Loss: 0.18449650704860687 \n",
      "Epoch 0 | step 133 Loss: 0.18956232070922852 \n",
      "Epoch 0 | step 134 Loss: 0.19199882447719574 \n",
      "Epoch 0 | step 135 Loss: 0.18815332651138306 \n",
      "Epoch 0 | step 136 Loss: 0.19706790149211884 \n",
      "Epoch 0 | step 137 Loss: 0.1813557893037796 \n",
      "Epoch 0 | step 138 Loss: 0.1917707920074463 \n",
      "Epoch 0 | step 139 Loss: 0.2118314504623413 \n",
      "Epoch 0 | step 140 Loss: 0.19604595005512238 \n",
      "Epoch 0 | step 141 Loss: 0.18140216171741486 \n",
      "Epoch 0 | step 142 Loss: 0.2022690325975418 \n",
      "Epoch 0 | step 143 Loss: 0.19386787712574005 \n",
      "Epoch 0 | step 144 Loss: 0.18854041397571564 \n",
      "Epoch 0 | step 145 Loss: 0.18336088955402374 \n",
      "Epoch 0 | step 146 Loss: 0.1932751089334488 \n",
      "Epoch 0 | step 147 Loss: 0.18732725083827972 \n",
      "Epoch 0 | step 148 Loss: 0.176413431763649 \n",
      "Epoch 0 | step 149 Loss: 0.18401765823364258 \n",
      "Epoch 0 | step 150 Loss: 0.18213403224945068 \n",
      "Epoch 0 | step 151 Loss: 0.18019302189350128 \n",
      "Epoch 0 | step 152 Loss: 0.19217641651630402 \n",
      "Epoch 0 | step 153 Loss: 0.1814456582069397 \n",
      "Epoch 0 | step 154 Loss: 0.17632298171520233 \n",
      "Epoch 0 | step 155 Loss: 0.1739189177751541 \n",
      "Epoch 0 | step 156 Loss: 0.1780332773923874 \n",
      "Epoch 0 | step 157 Loss: 0.18193550407886505 \n",
      "Epoch 0 | step 158 Loss: 0.18031518161296844 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rushi\\Downloads\\q2_diff.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rushi/Downloads/q2_diff.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, T, (\u001b[39m128\u001b[39m,))\u001b[39m.\u001b[39mlong()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rushi/Downloads/q2_diff.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m x_noisy,noise \u001b[39m=\u001b[39mforward_diffusion_sample(batch[\u001b[39m0\u001b[39m],t)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rushi/Downloads/q2_diff.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m noise_pred \u001b[39m=\u001b[39m model(x_noisy,t)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rushi/Downloads/q2_diff.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss\u001b[39m=\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39ml1_loss(noise,noise_pred)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rushi/Downloads/q2_diff.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\rushi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rushi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\rushi\\Downloads\\q2_diff.ipynb Cell 7\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rushi/Downloads/q2_diff.ipynb#W6sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m residual_inputs \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rushi/Downloads/q2_diff.ipynb#W6sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39mfor\u001b[39;00m down \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdowns:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rushi/Downloads/q2_diff.ipynb#W6sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m     x \u001b[39m=\u001b[39m down(x, t)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rushi/Downloads/q2_diff.ipynb#W6sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     residual_inputs\u001b[39m.\u001b[39mappend(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rushi/Downloads/q2_diff.ipynb#W6sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39mfor\u001b[39;00m up \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mups:\n",
      "File \u001b[1;32mc:\\Users\\rushi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rushi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\rushi\\Downloads\\q2_diff.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rushi/Downloads/q2_diff.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m h \u001b[39m=\u001b[39m h \u001b[39m+\u001b[39m time_emb\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rushi/Downloads/q2_diff.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Second Conv\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rushi/Downloads/q2_diff.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbnorm2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(h)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rushi/Downloads/q2_diff.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Down or Upsample\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rushi/Downloads/q2_diff.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(h)\n",
      "File \u001b[1;32mc:\\Users\\rushi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rushi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rushi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\rushi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model=SimpleUnet()\n",
    "\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs=10\n",
    "loss_epoch=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_each=[]\n",
    "\n",
    "    for step,batch in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        t = torch.randint(0, T, (128,)).long()\n",
    "\n",
    "        x_noisy,noise =forward_diffusion_sample(batch[0],t)\n",
    "        noise_pred = model(x_noisy,t)\n",
    "\n",
    "        loss=nn.functional.l1_loss(noise,noise_pred)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_each.append(loss.item())\n",
    "        print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
    "    \n",
    "    loss_epoch.append(sum(loss_each)/len(loss_each))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleUnet(\n",
       "  (time_mlp): Sequential(\n",
       "    (0): SinusoidalPositionEmbeddings()\n",
       "    (1): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (downs): ModuleList(\n",
       "    (0): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=128, bias=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=256, bias=True)\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=512, bias=True)\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bnorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=1024, bias=True)\n",
       "      (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): Conv2d(1024, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bnorm2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (ups): ModuleList(\n",
       "    (0): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=512, bias=True)\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bnorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=256, bias=True)\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=128, bias=True)\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (time_mlp): Linear(in_features=32, out_features=64, bias=True)\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (transform): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bnorm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (output): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "save_dir = \"./images/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate and save 10 new images\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        # Randomly sample a timestep\n",
    "        t_sample = torch.randint(0, T, (1,)).long()\n",
    "\n",
    "        # Generate a noisy image\n",
    "        x_noisy_sample, _ = forward_diffusion_sample(batch[0], t_sample)\n",
    "\n",
    "        # Generate a denoised image using the trained model\n",
    "        denoised_image = model(x_noisy_sample, t_sample)\n",
    "\n",
    "        # Convert the images to numpy arrays\n",
    "        x_noisy_sample_np = x_noisy_sample[0].cpu().numpy().transpose(1, 2, 0)\n",
    "        denoised_image_np = denoised_image[0].cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "        # Plot and save the images\n",
    "        plt.figure(figsize=(8, 4))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow((x_noisy_sample_np + 1) / 2)  # Rescale to [0, 1]\n",
    "        plt.title(\"Noisy Image\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow((denoised_image_np + 1) / 2)  # Rescale to [0, 1]\n",
    "        plt.title(\"Denoised Image\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Save the images\n",
    "        plt.savefig(os.path.join(save_dir, f\"generated_image_{i+1}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "# Set the model back to training mode\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('images', exist_ok=True)\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        noise = torch.randn(1, nz, 1, 1)\n",
    "        fake = netG(noise).detach().cpu()\n",
    "        plt.figure(figsize=(1,1))\n",
    "        plt.axis(\"off\")\n",
    "        plt.savefig('images/fake_temp_image_%d.png' % i)\n",
    "        plt.imshow(np.transpose(fake[0],(1,2,0)))\n",
    "        plt.savefig('images/fake_image_%d.png' % i)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "from torchvision.models.inception import inception_v3\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def inception_score(imgs, cuda=False, batch_size=32, resize=False, splits=1):\n",
    "    \"\"\"Computes the inception score of the generated images imgs\n",
    "\n",
    "    imgs -- Torch dataset of (3xHxW) numpy images normalized in the range [-1, 1]\n",
    "    cuda -- whether or not to run on GPU\n",
    "    batch_size -- batch size for feeding into Inception v3\n",
    "    splits -- number of splits\n",
    "    \"\"\"\n",
    "    N = len(imgs)\n",
    "    # print(imgs)\n",
    "\n",
    "    assert batch_size > 0\n",
    "    assert N > batch_size\n",
    "\n",
    "    # Set up dtype\n",
    "    if cuda:\n",
    "        dtype = torch.cuda.FloatTensor\n",
    "    else:\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"WARNING: You have a CUDA device, so you should probably set cuda=True\")\n",
    "        dtype = torch.FloatTensor\n",
    "\n",
    "    # Set up dataloader\n",
    "    dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n",
    "\n",
    "    # Load inception model\n",
    "    inception_model = inception_v3(pretrained=True, transform_input=False).type(dtype)\n",
    "    inception_model.eval();\n",
    "    up = nn.Upsample(size=(299, 299), mode='bilinear').type(dtype)\n",
    "    \n",
    "    def get_pred(x):\n",
    "        if resize:\n",
    "            x = up(x)\n",
    "        x = inception_model(x)\n",
    "        return F.softmax(x).data.cpu().numpy()\n",
    "\n",
    "    # Get predictions\n",
    "    preds = np.zeros((N, 1000))\n",
    "\n",
    "    for i, batch in enumerate(dataloader, 0):\n",
    "        batch = batch.type(dtype)\n",
    "        batchv = Variable(batch)\n",
    "        batch_size_i = batch.size()[0]\n",
    "\n",
    "        preds[i*batch_size:i*batch_size + batch_size_i] = get_pred(batchv)\n",
    "\n",
    "    # Now compute the mean kl-div\n",
    "    split_scores = []\n",
    "\n",
    "    for k in range(splits):\n",
    "        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n",
    "        py = np.mean(part, axis=0)\n",
    "        scores = []\n",
    "        for i in range(part.shape[0]):\n",
    "            pyx = part[i, :]\n",
    "            scores.append(entropy(pyx, py))\n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "\n",
    "    return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "# if _name_ == '_main_':\n",
    "#     class IgnoreLabelDataset(torch.utils.data.Dataset):\n",
    "#         def _init_(self, orig):\n",
    "#             self.orig = orig\n",
    "\n",
    "#         def _getitem_(self, index):\n",
    "#             return self.orig[index][0]\n",
    "\n",
    "#         def _len_(self):\n",
    "#             return len(self.orig)\n",
    "\n",
    "    # import torchvision.datasets as dset\n",
    "    # import torchvision.transforms as transforms\n",
    "\n",
    "    # cifar = dset.CIFAR10(root='data/', download=True,\n",
    "    #                          transform=transforms.Compose([\n",
    "    #                              transforms.Resize(32),\n",
    "    #                              transforms.ToTensor(),\n",
    "    #                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    #                          ])\n",
    "    # )\n",
    "\n",
    "    # IgnoreLabelDataset(cifar)\n",
    "\n",
    "    # print (\"Calculating Inception Score...\")\n",
    "    # print (inception_score(IgnoreLabelDataset(cifar), cuda=False, batch_size=32, resize=True, splits=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "def apply_transform_to_image(img_paths, transform):\n",
    "    result=[]\n",
    "    for img_path in img_paths:\n",
    "        pil_image = Image.open(img_path)\n",
    "        img_rgb = pil_image.convert('RGB')\n",
    "        transformed_image = transform(img_rgb)\n",
    "        result.append(transformed_image)\n",
    "    return torch.stack(result)\n",
    "\n",
    "# Example usage\n",
    "image_paths = ['./images/fake_image_0.png', './images/fake_image_1.png', './images/fake_image_2.png', './images/fake_image_3.png','./images/fake_image_4.png', './images/fake_image_5.png', './images/fake_image_6.png', './images/fake_image_7.png', './images/fake_image_8.png', './images/fake_image_9.png']\n",
    "\n",
    "# Define the transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "transformed_images = apply_transform_to_image(image_paths,transform)\n",
    "\n",
    "a,b=inception_score(transformed_images, cuda=False, batch_size=2, resize=True, splits=10)\n",
    "print(a,b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
